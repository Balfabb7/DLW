{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that all libraries are installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset from to the relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train images shape: \n",
      "(60000, 28, 28)\n",
      "[5 0 4 ... 5 6 8]\n",
      "\n",
      "test images shape: \n",
      "(10000, 28, 28)\n",
      "[7 2 1 ... 4 5 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "( train_images, train_labels ), ( test_images, test_labels ) = mnist.load_data()\n",
    "\n",
    "#outputting the shape of the data\n",
    "print( '' )\n",
    "print( 'train images shape: ' )\n",
    "print( train_images.shape )\n",
    "len( train_labels )\n",
    "print( train_labels )\n",
    "print( '' )\n",
    "\n",
    "print( 'test images shape: ' )\n",
    "print( test_images.shape )\n",
    "len( test_labels )\n",
    "print( test_labels )\n",
    "print( '' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a random image in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow( train_images[ 0 ], cmap = 'gray' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print( train_labels[ 5 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with reshaping the data for the convnet.\n",
    "\n",
    "notice we changed up the hyperparameters for this.\n",
    "Rather than using reshape: ( 60000, 28 * 28 ).\n",
    "We instead used: ( 60000, 28, 28, 1 ).\n",
    "\n",
    "This is because we need this format to fit the input shape of the model.\n",
    "\n",
    "Other than that we are still operating on the premise:\n",
    "\n",
    "1. shaping the data for the model.\n",
    "2. encoding the labels for both training and testing sets.\n",
    "3. compiling and training the model with training data.\n",
    "4. evaluating the model with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape( ( 60000, 28, 28, 1 ) )\n",
    "train_images = train_images.astype( 'float32' ) / 255\n",
    "test_images = test_images.reshape( ( 10000, 28, 28, 1 ) )\n",
    "test_images = test_images.astype( 'float32' ) / 255\n",
    "train_labels = to_categorical( train_labels )\n",
    "test_labels = to_categorical( test_labels )\n",
    "print( train_labels.shape )\n",
    "print( train_labels[ 5 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the convolutional network. While you could use just the Dense layers to achieve digit identification. You could gain even more accuracy if you use convolutional network. So the idea is to use a filter that will create localized values which will piece together to identify digits.\n",
    "\n",
    "So here we have a couple of functions from layers:\n",
    "\n",
    "Conv2D is the convolutional layer.\n",
    "Conv2D( filters, ( filter_height, filter_width ), ( activation ), ( input_shape ) )\n",
    "\n",
    "The filter will move through the image pixel by pixel ( or \"convolves\") around the image picking up values.\n",
    "This will create an output of: 26 x 26.\n",
    "\n",
    "MaxPooling2D( height, width )\n",
    "MaxPooling layer just takes the highest values from the Convolutional layer and puts it in height x width. \n",
    "It will use a filter of 2 x 2 moving around the 26 x 26 output with the purpose of taking the highest values from the 4 squares and create an output of 1 value each stride which because the filter is 2 x 2, the stride would also be 2. Note that MaxPooling will reduce the output to half.\n",
    "\n",
    "The last layer: model.add( layers.Dense( 10. activation = 'softmax' )\n",
    "\n",
    "So here we have 10 neurons which will associate, imagine digits between 0 and 9. Each neuron will contain values be between 0 - 1 and all of the values will sum to 1.\n",
    "\n",
    "So basically we have a probability for a digit prediction. The highest probability will be the model's prediction for the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 4)         580       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 484)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                4850      \n",
      "=================================================================\n",
      "Total params: 5,590\n",
      "Trainable params: 5,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add( layers.Conv2D( 32, ( 3, 3 ), activation = 'relu', input_shape=( 28, 28, 1 ) ) )\n",
    "model.add( layers.MaxPooling2D( ( 2, 2 ) ) )\n",
    "model.add( layers.Conv2D( 64, ( 3 ,3 ), activation = 'relu' ) )\n",
    "model.add( layers.MaxPooling2D( ( 2, 2 ) ) )\n",
    "model.add( layers.Conv2D( 64, ( 3, 3 ), activation = 'relu' ) )\n",
    "model.add( layers.Flatten() )\n",
    "model.add( layers.Dense( 64, activation = 'relu' ) )\n",
    "model.add( layers.Dense( 10, activation = 'softmax' ) )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compiling and training the model\n",
    "\n",
    "The model will be trained with the fit function.\n",
    "\n",
    "RMSprop is root mean square propagation.\n",
    "\n",
    "categorical crossentropy for multi class classification.\n",
    "\n",
    "We load in the training images, training labels with 5 epochs. Normally batch size default is 32, here we are using 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 33s 555us/step - loss: 0.3788 - acc: 0.8892\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 36s 598us/step - loss: 0.1439 - acc: 0.9580\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 38s 638us/step - loss: 0.1090 - acc: 0.9682\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 41s 681us/step - loss: 0.0934 - acc: 0.9726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227ff71bbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile( optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = [ 'accuracy' ] )\n",
    "model.fit( train_images, train_labels, epochs = 5, batch_size = 64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 243us/step\n",
      "test_acc: 0.9763\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model with the test data now\n",
    "test_loss, test_acc = model.evaluate( test_images, test_labels )\n",
    "print( 'test_acc:', test_acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are visualzing the data for the test images. \n",
    "\n",
    "The test_images paramaters are different because we are calling a reshaped test_images, so the parameters have to match accordingly.\n",
    "\n",
    "Notice the test_labels is also reshaped. Rather than seeing a value of 0, we get an array of 10 in binary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "plt.matshow( test_images[ 0, :, :, 0 ], cmap = 'gray' )\n",
    "plt.show()\n",
    "print( test_labels[ 0 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the predictions predictions on the unseen data\n",
    "This is going to show us how the model is predicting the images.\n",
    "\n",
    "The output is an array of values where each value is associated to a digit.\n",
    "\n",
    "If we summed up all the neurons we would get a sum of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.3754508e-08 2.1981594e-09 3.6539913e-07 1.4834933e-05 8.7365132e-12\n",
      " 3.8641147e-08 2.9599114e-16 9.9998152e-01 5.8638523e-08 3.0531103e-06]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict( test_images )\n",
    "print( predictions[ 0 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rounding we can see the output of the prediction. Here we see how the model predicts which digit which is in binary format of array 0 - 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print( predictions[ 0 ].round() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
